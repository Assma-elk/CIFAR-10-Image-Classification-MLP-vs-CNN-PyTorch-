# CIFAR-10-Image-Classification-MLP-vs-CNN-PyTorch-
This project explores image classification on the CIFAR-10 dataset using two different neural network architectures: a **Multi-Layer Perceptron (MLP)** and a **Convolutional Neural Network (CNN)**. The goal is to highlight the limitations of fully connected models on image data and the benefits of convolutional inductive bias.
---

## ðŸ“Š Dataset

**CIFAR-10** is a standard computer vision dataset composed of:
- 60,000 color images (32Ã—32)
- 10 classes (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck)
- 50,000 training images / 10,000 test images

---

## ðŸ§  Models

### ðŸ”¹ MLP (Baseline)
- Fully connected neural network
- Images flattened into vectors
- Used as a baseline to evaluate performance without spatial information

### ðŸ”¹ CNN
- Convolutional layers to capture spatial patterns
- Max-pooling layers for dimensionality reduction
- Data augmentation (random crops, horizontal flips)
- Weight decay and learning rate scheduling for regularization

---

## ðŸ‹ï¸ Training Pipeline

- PyTorch framework
- Reproducibility ensured via fixed random seeds
- Train / validation / test split
- Optimizer: Adam
- Learning rate scheduler
- Accuracy used as main evaluation metric

---

## ðŸ“ˆ Results (typical)

| Model | Test Accuracy |
|------|---------------|
| MLP  | ~45â€“50% |
| CNN  | ~70â€“75% |

> The CNN significantly outperforms the MLP, which is expected for image-based tasks.



## ðŸ§© Key Takeaways

- Fully connected networks are poorly suited for image data
- Convolutional layers effectively exploit spatial structure
- Data augmentation improves generalization
- Regularization and learning rate scheduling have a noticeable impact on performance


## ðŸ›  Tech Stack

- Python
- PyTorch
- NumPy
- Matplotlib


